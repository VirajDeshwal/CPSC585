{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropogation using real-valued computational graph \n",
    "\n",
    "```\n",
    " dQ/dX\n",
    "X ---\\\n",
    "   w1   \\    df/dQ\n",
    "       (+)---\\\n",
    " dQ/dY/   Q w4  \\\n",
    "Y ---/         \\\n",
    "   w2             (+)--- f(x, y, z)\n",
    "               /   df/df\n",
    " df/dZ        /\n",
    "Z -----w3------/\n",
    "```\n",
    "\n",
    "#### By Chain rule\n",
    "```\n",
    "given Q = w1X + w2Y\n",
    "\n",
    "df/dQ = df/df * df/dQ\n",
    "df/dX = df/dQ * dQ/dX\n",
    "df/dY = df/dQ * dQ/dY\n",
    "df/dZ = df/df * df/dZ\n",
    "```\n",
    "by manually calculating the partial derivatives...\n",
    "```\n",
    "df/dQ = 1\n",
    "df/dX = w1\n",
    "df/dY = w2\n",
    "df/dZ = 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(w1,w2,w3,w4):\n",
    "    return w4*(w1*x+w2*y)+w3*z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward(w1,w2,w3,w4):\n",
    "    dfdw1 =w4*x\n",
    "    dfdw2 = w4*y\n",
    "    dfdw3 = z\n",
    "    dfdw4 = (w1*x +w2*y)\n",
    "    return dfdw1, dfdw2, dfdw3, dfdw4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients on w1: 12, w2: 30, w3: 9 and w4: 12\n"
     ]
    }
   ],
   "source": [
    "x = 2\n",
    "w1=1\n",
    "y = 5\n",
    "w2=2\n",
    "z = 9\n",
    "w3=3\n",
    "w4=6\n",
    "\n",
    "dw1, dw2, dw3, dw4 = backward(w1,w2,w3,w4)\n",
    "print (\"Gradients on w1: {}, w2: {}, w3: {} and w4: {}\".format(dw1, dw2, dw3,dw4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Converting the above computation graph to an optimization problem\n",
    "\n",
    "### This can simply be done by adding a loss layer at the end. Lets say we add a mean square loss `node` after `f(x, y, z)`. Here we define value `V` that we want to optimize using the function `f(x, y, z)`. Below is how the new computational graph would look like. Notice that we are taking derivatives w.r.t `L` not `f` anymore.\n",
    "\n",
    "```\n",
    " dQ/dX\n",
    "X ---\\\n",
    "      \\    df/dQ\n",
    "       (+)---\\\n",
    " dQ/dY/   Q   \\\n",
    "Y ---/         \\\n",
    "                (+)--- f(x, y, z)---Loss\n",
    "               /   dL/df\n",
    "        df/dZ /\n",
    "Z -----------/\n",
    "```\n",
    "\n",
    "```\n",
    "L = (V - f(x,y,z))^2     where f(x,y,z) = w4(w1x+w2y)+w3*z\n",
    "dL/dQ = dL/df * df/dQ\n",
    "dL/dw1 = dL/dQ * dQ/dw1\n",
    "dL/dw2 = dL/dQ * dQ/dw2\n",
    "dL/dw3 = dL/df * df/dw3\n",
    "dL/dw4 = dL/df * df/dw4\n",
    "\n",
    "```\n",
    "\n",
    "analytically this becomes...\n",
    "\n",
    "```\n",
    "dL/dw1 = -2(V-(w4(w1x+w2y)+w3*z)*w4x\n",
    "dL/dw2 = -2(V-(w4(w1x+w2y)+w3*z)*w4y\n",
    "dL/dw3 = -2(V-(w4(w1x+w2y)+w3*z)*z\n",
    "dL/dw4= -2(V-(w4(w1x+w2y)+w3*z)*(w1x+w2y)\n",
    "\n",
    "```\n",
    "\n",
    "now the backward calculation changes to below. You can verify this using wolframalpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_w_loss(V, w1,w2,w3,w4):\n",
    "    dldw1 = -2*(V-(w4*(w1*x+w2*y)+w3*z))*w4*x\n",
    "    dldw2 = -2*(V-(w4*(w1*x+w2*y)+w3*z))*w4*y\n",
    "    dldw3 = -2*(V-(w4*(w1*x+w2*y)+w3*z))*z\n",
    "    dldw4 = -2*(V-(w4*(w1*x+w2*y)+w3*z))*(w1*x+w2*y)\n",
    "    return dldw1, dldw2, dldw3, dldw4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization loop for variables `x`, `y` and `z`\n",
    "\n",
    "Here I am optimizing the variables to produce a `final_value` of `145`. Which is partly equivalent to training a neural network to learn a certain output/final_value. Notice how in the iteration, you do not need the `forward` function (only in this case because it is a simple/small network and we are solving the entire network analytically). If you were do this piecewise as in a `real` neural network, you need to do the `forward` pass to calculate the loss, which will then get backpropogated through the network.\n",
    "\n",
    "Play around with all initial values of the variables (long enough) and you will essentially find all the real values of `x`, `y` and `z` that satisfy the equation. \n",
    "\n",
    "Notice that this problem has the same steps as the previous curvefitting assigment. Here instead you are training a computational graph to approximate an output value. Also a more complex computational graph like a neural network will follow the same steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Values of dw1: 1.4927532249355941, dw2: 3.231883062338986, dw3: 8.466380584931686, and z: 3.593776751479741\n",
      "Evaluation of (x + y) + z = 145.00000000000006\n"
     ]
    }
   ],
   "source": [
    "x = 2\n",
    "w1=1\n",
    "y = 5\n",
    "w2=2\n",
    "z = 9\n",
    "w3=3\n",
    "w4=6\n",
    "\n",
    "\n",
    "alpha = 0.001\n",
    "final_value = 145\n",
    "n = 10000\n",
    "for i in range(n):\n",
    "    dw1, dw2, dw3,dw4 = backward_w_loss(final_value, w1, w2, w3, w4)\n",
    "    w1 = w1 - alpha * dw1\n",
    "    w2 = w2 - alpha * dw2\n",
    "    w3 = w3 - alpha * dw3\n",
    "    w4 = w4 - alpha * dw4\n",
    "print (\"Final Values of dw1: {}, dw2: {}, dw3: {}, and z: {}\".format(w1, w2, w3, w4))\n",
    "# forward is only used here below\n",
    "print (\"Evaluation of (x + y) + z = {}\".format(forward(w1, w2, w3, w4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
